#!/usr/bin/env node
@import '../share/channel.jmacs'

const factory = require('@{channel('core.data.factory')}');

const gobble = (s_text, s_indent='') => {
	let m_pad = /^(\s+)/.exec(s_text.replace(/^([ \t]*\n)/, ''));
	if(m_pad) {
		return s_indent+s_text.replace(new RegExp(`\\n${m_pad[1]}`, 'g'), '\n'+s_indent.trim()).trim();
	}
	else {
		return s_indent+s_text.trim();
	}
};

@.{
	let h_content_types = {
		ttl: {},
		trig: {},
		nt: {},
		nq: {},
	};
}

@> content(s_content)

	// @{s_content} package
	const @{s_content} = {
		// read @{s_content} output
		get read() {
			// memoize
			delete @{s_content}.read;
			return @{s_content}.read = require('@{channel(`content.${s_content}.read`)}');  // eslint-disable-line global-require
		},

		// scan @{s_content} output
		get scan() {
			// memoize
			delete @{s_content}.scan;
			return @{s_content}.scan = require('@{channel(`content.${s_content}.scan`)}');  // eslint-disable-line global-require
		},

		// write @{s_content} input
		get write() {
			// memoize
			delete @{s_content}.write;
			return @{s_content}.write = require('@{channel(`content.${s_content}.write`)}');  // eslint-disable-line global-require
		},

		// scribe @{s_content} input
		get scribe() {
			// memoize
			delete @{s_content}.scribe;
			return @{s_content}.scribe = require('@{channel(`content.${s_content}.scribe`)}');  // eslint-disable-line global-require
		},
	};
@;

@*{
	for(let s_content in h_content_types) {
		yield content(s_content);
	}
}


// // SPARQL Results package
// const sparql_results = {
// 	// deserialize sparql_results input
// 	get deserializer() {
// 		// memoize
// 		delete sparql_results.deserializer;
// 		return (sparql_results.deserializer = require('../sparql-results/deserializer.js'));
// 	},
// };


const H_CONTENT_MIMES = {
	'text/turtle': ttl,
	'application/trig': trig,
	'application/n-triples': nt,
	'application/n-quads': nq,
	// 'application/sparql-results+json': sparql_results,
};

const H_CONTENT_TAGS = {
	ttl,
	trig,
	nt,
	nq,
	// 'application/sparql-results+json': sparql_results,
};


@> memoize(s_package)
	@//@object-literal
	get @{s_package.replace(/^.*\.([^.]+)$/, '$1')}() {
		@//@
		// memoize
		delete graphy.@{s_package};
		return (graphy.@{s_package} = require('@{channel(s_package)}'));
	},
@;

@.{
	let h_standalones = {
		bat: {},
		set: {},
		viz: {},
		// store: {},
	};
}

@>> R_TOKENS()
	@//@regex
	[^\0-\x20()<>@,;:\\"\/[\]?.=]+
@;

@//@
const R_CONTENT_TYPE = /^((?:application|text)\/@{R_TOKENS()})(;.+)*$/i;

const graphy = module.exports = Object.assign({

	VERSION: '4.0.0',

	content: Object.assign(function(s_query) {
		if(s_query in H_CONTENT_TAGS) {
			return H_CONTENT_TAGS[s_query];
		}

		let m_content_type = R_CONTENT_TYPE.exec(s_query);
		if(!m_content_type) throw new Error(`invalid content-type string: "${s_query}"`);
		let [, s_content_type, s_parameters] = m_content_type;
		let s_content_type_normal = s_content_type.toLowerCase();

		if(s_content_type_normal in H_CONTENT_MIMES) {
			return H_CONTENT_MIMES[s_content_type_normal];
		}
		else {
			throw new Error(`no content handlers matched query for "${s_content_type_normal}"`);
		}
	}, {
		@*{
			for(let s_content in h_content_types) {
				yield /* syntax: js.object-literal */ `${s_content},\n`;
			}
		}
	}),

	core: {
		data: {
			@{memoize('core.data.factory')}
		},
	},

	get 'core.data.factory'() {
		delete graphy['core.data.factory'];
		return (graphy['core.data.factory'] = require('@{channel('core.data.factory')}'));
	},

	get 'util.dataset.tree'() {
		delete graphy['util.dataset.tree'];
		return (graphy['util.dataset.tree'] = require('@{channel('util.dataset.tree')}'));
	},

	util: {
		dataset: {
			@{memoize('util.dataset.tree')}
		},
	},

	@*{
		for(let s_content in h_content_types) {
			yield /* syntax: js.object-literal */ `
				get 'content.${s_content}.read' () {
					// memoize
					delete graphy['content.${s_content}.read'];
					return graphy['content.${s_content}.read'] = require('${channel(`content.${s_content}.read`)}');  // eslint-disable-line global-require
				},
			`;

			yield /* syntax: js.object-literal */ `
				get 'content.${s_content}.write' () {
					// memoize
					delete graphy['content.${s_content}.write'];
					return graphy['content.${s_content}.write'] = require('${channel(`content.${s_content}.write`)}');  // eslint-disable-line global-require
				},
			`;

			yield /* syntax: js.object-literal */ `
				get 'content.${s_content}.scribe' () {
					// memoize
					delete graphy['content.${s_content}.scribe'];
					return graphy['content.${s_content}.scribe'] = require('${channel(`content.${s_content}.scribe`)}');  // eslint-disable-line global-require
				},
			`;
		}
	}


@//@object-literal

}, factory);

@//@

// export graphy to window object if in main thread of browser
if('undefined' !== typeof window) window.graphy = graphy;

// cli
if(module === require.main) {
	const fs =require('fs');
	const path = require('path');
	const mk_yargs = require('yargs/yargs');
	const stream = require('@{channel('core.iso.stream')}');

	const parse_filter = require('./quad-expression.js').parse;
	const expression_handler = require('./expression-handler.js');

	const F_ADAPT_STREAM = function(ds_out) {
		let ds_dst = ds_out;

		// non-object mode
		if(!ds_dst._writableState.objectMode) {
			// transform to JSON
			ds_out = stream.quads_to_json();
		}
		// yes object mode and graphy writable
		else if(ds_out.isGraphyWritable) {
			// transform to writable data events
			ds_out = stream.quads_to_writable();
		}
		// forward as-is to super
		else {
			return this.constructor.prototype.pipe.call(this, ds_dst);
		}

		// forward output to super
		this.constructor.prototype.pipe.call(this, ds_out);

		// pipe output to destination
		return ds_out.pipe(ds_dst);
	};

	const bypass = a_inputs => a_inputs.map((ds_input) => {
		// intercept pipe
		ds_input.pipe = F_ADAPT_STREAM;

		return ds_input;
	});

	const warp_term = (z_term, h_prefixes) => {
		// c1 string
		if('string' === typeof z_term) {
			return factory.c1(z_term, h_prefixes);
		}
		// normalize term
		else {
			return factory.fromTerm(z_term);
		}
	};

	const interpret_item = (z_item, h_prefixes, ds_transform, fke_transform) => {
		// array
		if(Array.isArray(z_item)) {
			// zero-length, skip
			if(!z_item.length) return fke_transform();

			// first object is also array
			if(Array.isArray(z_item[0])) {
				let nl_subs = z_item.length;

				let c_resolves = 0;

				for(let z_sub of z_item) {
					interpret_item(z_sub, h_prefixes, ds_transform, () => {  // eslint-disable-line no-loop-func
						if(++c_resolves === nl_subs) {
							fke_transform();
						}
					});
				}

				// do not consume transform synchronously
				return;
			}
			// triple/quad
			else if(3 === z_item.length || 4 === z_item.length) {
				let a_terms = z_item.map(z => warp_term(z, h_prefixes));

				ds_transform.push(factory.quad(...a_terms));
			}
		}
		// string (trig)
		else if('string' === typeof z_item) {
			trig.read({
				input: {
					string: z_item,

					error(e_read) {
						warn(`The 'transform' command threw an Error while trying to read the returned TriG string: '${z_item}'\n\nThe reader reported: ${e_read.stack}`);

						// done
						fke_transform();
					},
				},

				data(g_quad_read) {
					ds_transform.push(g_quad_read);
				},

				eof() {
					// done
					fke_transform();
				},
			});

			// do not consume transform synchronously
			return;
		}
		// quad
		else if(z_item.subject && z_item.predicate && z_item.object) {
			ds_transform.push(factory.fromQuad(z_item));
		}
		// iterable
		else if(z_item[Symbol.iterator]) {
			for(let g_quad_it of z_item) {
				ds_transform.push(g_quad_it);
			}
		}
		// other
		else {
			exit(`The callback function supplied to the 'transform' command returned an invalid quad value: '${z_item}'`);
		}

		// done
		fke_transform();
	};

	class answer_source extends require('stream').Readable {
		constructor(w_datum) {
			super({
				objectMode: true,
			});

			this.datum = w_datum;
		}

		// intercept pipe
		pipe(ds_dst) {
			// string out
			if(!ds_dst._writableState.objectMode) {
				// change read mode; push as JSON
				this._read = () => {
					this.push(JSON.stringify(this.datum)+'\n', 'utf8');
					this.push(null);
				};
			}

			// forward to super
			return super.pipe(ds_dst);
		}

		// push object
		_read() {
			this.push(this.datum);
			this.push(this.null);
		}
	}

	const warn = (s_message) => {
		console.warn((new Error(s_message)).stack
			.replace(/\n\s+at [^\n]*\n/, '\n')
			.replace(/^Error:/, 'Warning:'));
	};

	const exit = (s_exit) => {
		console.error(s_exit);
		process.exit(1);
	};

	const command = s_command => mk_yargs()
		.strict()
		.usage(`Usage: $0 ${s_command} [OPTIONS] [--pipe COMMAND]`);

	const reader = f_reader => (a_args, g_context) => new Promise((fk_resolve) => {
		let g_argv = command(g_context.command)
			.options({
				v: {
					type: 'boolean',
					alias: ['validate'],
					default: undefined,  // eslint-disable-line no-undefined
					describe: 'DEPRECATED. Validation is now enabled by default',
				},
				r: {
					type: 'boolean',
					alias: ['relax'],
					default: undefined,  // eslint-disable-line no-undefined
					describe: 'relax validation of tokens within the RDF document',
				},
				b: {
					type: 'string',
					alias: ['base', 'base-uri', 'base-iri'],
					describe: 'set a base URI on the document',
				},
				s: {
					type: 'string',
					alias: ['subject'],
					describe: 'DEPRECATED. Use \'filter\' command instead. Filter quads by only allowing those that match the given subject (must be a concise-term string)',
					conflicts: ['S'],
				},
				p: {
					type: 'string',
					alias: ['predicate'],
					describe: 'DEPRECATED. Use \'filter\' command instead. Filter quads by only allowing those that match the given predicate (must be a concise-term string)',
					conflicts: ['P'],
				},
				o: {
					type: 'string',
					alias: ['object'],
					describe: 'DEPRECATED. Use \'filter\' command instead. Filter quads by only allowing those that match the given object (must be a concise-term string)',
					conflicts: ['O'],
				},
				g: {
					type: 'string',
					alias: ['graph'],
					describe: 'DEPRECATED. Use \'filter\' command instead. Filter quads by only allowing those that match the given graph (must be a concise-term string)',
					conflicts: ['G'],
				},
				S: {
					type: 'array',
					alias: ['not-subject'],
					describe: 'DEPRECATED. Use \'filter\' command instead. Filter quads by allowing any that *do not match* the given subject(s) (must be concise-term string(s))',
					conflicts: ['s'],
				},
				P: {
					type: 'array',
					alias: ['not-predicate'],
					describe: 'DEPRECATED. Use \'filter\' command instead. Filter quads by allowing any that *do not match* the given predicate(s) (must be a concise-term string(s))',
					conflicts: ['p'],
				},
				O: {
					type: 'array',
					alias: ['not-object'],
					describe: 'DEPRECATED. Use \'filter\' command instead. Filter quads by allowing any that *do not match* the given object(s) (must be a concise-term string(s))',
					conflicts: ['o'],
				},
				G: {
					type: 'array',
					alias: ['not-graph'],
					describe: 'DEPRECATED. Use \'filter\' command instead. Filter quads by allowing any that *do not match* the given graph(s) (must be a concise-term string(s))',
					conflicts: ['g'],
				},
			})
			.help()
			.version(false)
			.parse(a_args);

		let gc_read = {
			relax: g_argv.relax || false,
		};

		// 'validate' option is set
		if('undefined' !== typeof g_argv.validate) {
			warn(`The 'validate' option has been deprecated. Validation is now enabled by default. Use the 'relax' option if you wish to disable validation.`);

			// infer validation value from flag
			if('undefined' === typeof g_argv.relax) {
				gc_read.relax = !g_argv.validate;
			}
		}

		// 'base-uri' => 'baseUri'
		if(g_argv['base-uri']) {
			gc_read.baseUri = g_argv['base-uri'];
		}

		fk_resolve(g_context.inputs.map((ds_input) => {
			let ds_reader = ds_input.pipe(f_reader({
				...gc_read,
				error(e_read) {
					g_context.failure(e_read);
				},
			}));
			// filters
			if(g_argv.subject || g_argv.predicate || g_argv.object || g_argv.graph
				|| g_argv['not-subject'] || g_argv['not-predicate'] || g_argv['not-object'] || g_argv['not-graph']) {
				let {
					subject: sc1_subject=null,
					predicate: sc1_predicate=null,
					object: sc1_object=null,
					graph: sc1_graph=null,
					'not-subject': a_not_subjects_src=null,
					'not-predicate': a_not_predicates_src=null,
					'not-object': a_not_objects_src=null,
					'not-graph': a_not_graphs_src=null,
				} = g_argv;

				// deprecation notice
				warn(`All content reader term filter flags have been deprecated. Use the 'filter' CLI command instead.`);

				// create sv1 test strings
				let sv1_subject = null;
				let sv1_predicate = null;
				let sv1_object = null;
				let sv1_graph = null;

				// create not filter arrays
				let a_not_subjects = null;
				let a_not_predicates = null;
				let a_not_objects = null;
				let a_not_graphs = null;

				// prefix mappings
				let h_prefixes = {};

				// graph optimzation
				let b_skip_graph = false;

				// skip graphs that do not match filter
				let f_enter = (yt_graph) => {
					b_skip_graph = (sv1_graph !== yt_graph.concise());
				};

				// only skip triples if filter is not the default graph
				let f_exit = () => {
					b_skip_graph = ('*' !== sv1_graph);
				};

				// skip graphs that match not filter
				let f_enter_not = (yt_graph) => {
					b_skip_graph = a_not_graphs.includes(yt_graph.concise());
				};

				// only skip triples if not filter is the default graph
				let f_exit_not = () => {
					b_skip_graph = a_not_graphs.includes('*');
				};

				// when prefix mappings change, populate filter strings
				let f_update_filters = () => {
					// filter subject
					if(sc1_subject) {
						try {
							sv1_subject = factory.c1_node(sc1_subject, h_prefixes).concise();
						}
						catch(e_convert) {
							if(!/prefix not defined/i.test(e_convert.message)) throw e_convert;
							sv1_subject = '`void';
						}
					}
					// filter not subject
					else if(a_not_subjects_src) {
						a_not_subjects = a_not_subjects_src.map((sc1_not_subject) => {
							try {
								return factory.c1_node(sc1_not_subject, h_prefixes).concise();
							}
							catch(e_convert) {
								if(!/prefix not defined/i.test(e_convert.message)) throw e_convert;
								return null;
							}
						});
					}

					// filter predicate
					if(sc1_predicate) {
						try {
							sv1_predicate = factory.c1_named_node(sc1_predicate, h_prefixes).concise();
						}
						catch(e_convert) {
							if(!/prefix not defined/i.test(e_convert.message)) throw e_convert;
							sv1_predicate = '`void';
						}
					}
					// filter not predicate
					else if(a_not_predicates_src) {
						a_not_predicates = a_not_predicates_src.map((sc1_not_predicate) => {
							try {
								return factory.c1_node(sc1_not_predicate, h_prefixes).concise();
							}
							catch(e_convert) {
								if(!/prefix not defined/i.test(e_convert.message)) throw e_convert;
								return null;
							}
						});
					}

					// filter object
					if(sc1_object) {
						try {
							sv1_object = factory.c1(sc1_object, h_prefixes).concise();
						}
						catch(e_convert) {
							if(!/prefix not defined/i.test(e_convert.message)) throw e_convert;
							sv1_object = '`void';
						}
					}
					// filter not object
					else if(a_not_objects) {
						a_not_objects = a_not_objects_src.map((sc1_not_object) => {
							try {
								return factory.c1(sc1_not_object, h_prefixes).concise();
							}
							catch(e_convert) {
								if(!/prefix not defined/i.test(e_convert.message)) throw e_convert;
								return null;
							}
						});
					}

					// filter graph
					if(sc1_graph) {
						// listeners already set, remove them
						if(sv1_graph) {
							ds_reader.removeListener('enter', f_enter);
							ds_reader.removeListener('exit', f_exit);
						}

						try {
							sv1_graph = factory.c1_node(sc1_graph, h_prefixes).concise();
						}
						catch(e_convert) {
							if(!/prefix not defined/i.test(e_convert.message)) throw e_convert;
							sv1_graph = '`void';
						}

						// setup graph filter
						ds_reader
							.on('enter', f_enter)
							.on('exit', f_exit);
					}
					// filter not graph
					else if(a_not_graphs_src) {
						// listeners might already set, remove them
						if(a_not_graphs) {
							ds_reader.removeListener('enter', f_enter_not);
							ds_reader.removeListener('exit', f_exit_not);
						}

						a_not_graphs = a_not_graphs_src.map((sc1_not_graph) => {
							try {
								return factory.c1_node(sc1_not_graph, h_prefixes).concise();
							}
							catch(e_convert) {
								if(!/prefix not defined/i.test(e_convert.message)) throw e_convert;
								return null;
							}
						});

						// setup graph filter
						ds_reader
							.on('enter', f_enter_not)
							.on('exit', f_exit_not);
					}
				};

				// listen for prefix events
				ds_reader.on('prefix', (s_prefix_id, p_prefix_iri) => {
					// update hash
					h_prefixes[s_prefix_id] = p_prefix_iri;

					// update filters
					f_update_filters();
				});

				// create filter transform
				let ds_filter = new (class extends stream.Transform {
					constructor() {
						super({
							objectMode: true,
						});
					}

					// eslint-disable-next-line class-methods-use-this
					_transform(y_quad, s_encoding, fk_transform) {
						// skip graph
						if(b_skip_graph) return fk_transform();

						// apply filter
						if(sv1_subject && sv1_subject !== y_quad.subject.concise()) return fk_transform();
						else if(sv1_predicate && sv1_predicate !== y_quad.predicate.concise()) return fk_transform();
						else if(sv1_object && sv1_object !== y_quad.object.concise()) return fk_transform();
						else if(sv1_graph && sv1_graph !== y_quad.graph.concise()) return fk_transform();

						// apply not filter
						if(a_not_subjects && a_not_subjects.includes(y_quad.subject.concise())) return fk_transform();
						else if(a_not_predicates && a_not_predicates.includes(y_quad.predicate.concise())) return fk_transform();
						else if(a_not_objects && a_not_objects.includes(y_quad.object.concise())) return fk_transform();
						else if(a_not_graphs && a_not_graphs.includes(y_quad.graph.concise())) return fk_transform();

						// quad passed filter
						fk_transform(null, y_quad);
					}

					// intercept pipe
					pipe(ds_out) {
						let ds_dst = ds_out;

						// non-object mode
						if(!ds_dst._writableState.objectMode) {
							// transform to JSON
							ds_out = stream.quads_to_json();
						}
						// yet object mode and graphy writable
						else if(ds_out.isGraphyWritable) {
							// transform to writable data events
							ds_out = stream.quads_to_writable();
						}

						// interim stream created
						if(ds_out !== ds_dst) {
							// forward output to super
							super.pipe(ds_out);

							// pipe outpu to destination
							return ds_out.pipe(ds_dst);
						}
						// forward as-is to super
						else {
							return super.pipe(ds_dst);
						}
					}
				})();

				// pipe thru filter
				return ds_reader.pipe(ds_filter);
			}
			// no filter
			else {
				return ds_reader;
			}
		}));
	});

	const writer = (f_writer, gc_write={}) => (a_args, g_context) => {
		let s_group_style = 'Style options:';
		let s_group_list = 'List structure options:'

		let g_argv = command(g_context.command)
			.options({
				i: {
					type: 'string',
					alias: ['indent'],
					// default: '\\t',  // eslint-disable-line no-undefined
					describe: `sets the whitespace string to use for indentation. Writers use '\\t' by default`,
					group: s_group_style,
				},
				g: {
					type: 'string',
					alias: ['graph-keyword'],
					describe: `sets the style to use when serializing the optional 'GRAPH' keyword in TriG. Writers omit this keyword by default.
						Passing 'true' or empty with this flag on is shorthand for the all-caps 'GRAPH' keyword`.replace(/\n\s*/g, ' '),
					group: s_group_style,
				},
				s: {
					type: 'boolean',
					alias: ['simplify-default-graph'],
					describe: 'if enabled, omits serializing the surrounding optional graph block for the default graph in TriG.',
					group: s_group_style,
				},
				f: {
					type: 'string',
					alias: ['first'],
					describe: `c1 string: sets the predicate to use for the 'first' relation when serializing list structures`,
					group: s_group_list,
				},
				r: {
					type: 'string',
					alias: ['rest'],
					describe: `c1 string: sets the predicate to use for the 'rest' relation when serializing list structures`,
					group: s_group_list,
				},
				n: {
					type: 'string',
					alias: ['nil'],
					describe: `c1 string: sets the predicate to use for the 'nil' relation when serializing list structures`,
					group: s_group_list,
				},
			})
			.help()
			.parse(a_args);

		// extend style options
		let g_style = gc_write.style || {};
		{
			// indent
			if(g_argv.indent) g_style.indent = g_argv.indent;

			// graph keyword
			if(g_argv['graph-keyword']) g_style.graph_keyword = g_argv['graph-keyword'];

			// simplify default graph
			if(g_argv['simplify-default-graph']) g_style.simplify_default_graph = g_argv['simplify-default-graph'];
		}

		// extend list options
		let g_lists = gc_write.lists || {};
		{
			// first
			if(g_argv.first) g_lists.first = g_argv.first;

			// rest
			if(g_argv.rest) g_lists.rest = g_argv.rest;

			// nil
			if(g_argv.nil) g_lists.nil = g_argv.nil;
		}

		// map input(s) to writer(s)
		return g_context.inputs.map((ds_input) => {
			let ds_writer = f_writer({
				...gc_write,

				style: g_style,

				lists: g_lists,

				error(e_write) {
					g_context.failure(e_write);
				},
			});

			return ds_input.pipe(ds_writer);
		});
	};

	// commands
	let h_commands = {  // eslint-disable-next-line quote-props
		// 'content': (a_args, g_context) => {
		// 	let g_argv = command(g_context.command)
		// 		.string('t')
		// 			.alias('t', ['type'])
		// 			.describe('t', 'argument to `super.content()`; either an RDF Content-Type or format selector')
		// 		.string('v')
		// 			.alias('v', 'verb')
		// 			.describe('v', 'which verb to access on the given content handler, e.g., `read`, `write`, etc.')
		// 		.help()
		// 		.version(false)
		// 		.parse(a_args);

		// },


		head(a_args, g_context) {
			
		},

		tail(a_args, g_context) {
			
		},

		// copy(a_args, g_context) {
			
		// },

		// append(a_args, g_context) {
			
		// },

		concat(a_args, g_context) {
			
		},

		filter(a_args, g_context) {
			let g_argv = command(g_context.command)
				.options({
					x: {
						type: 'string',
						alias: ['expression'],
						describe: 'filter quads using the given quad filter expression',
						conflicts: ['j'],
						example: [
							`filter -x '; a; dbo:Plant'`,
							`filter -x 'dbr:Banana;; !{literal}'`,
						].join('\n'),
					},

					j: {
						type: 'string',
						alias: ['js', 'javascript'],
						describe: 'filter quads using the given JavaScript expression which will be evaluated as a callback function passed the quad and current prefix map as arguments',
						conflicts: ['x'],
						example: [
							`filter -j 'g => g.object.number > 10e3'`,
							`filter -j 'g => g.object.value.startsWith(g.subject.value)'`,
							`filter -j '(g, h) => g.subject.concise(h).startsWith("db")'`,
						].join('\n'),
					},

					v: {
						type: 'boolean',
						alias: ['verbose'],
						describe: 'prints the compiled quad filter expression to stderr',
					},
				})
				.help()
				.parse(a_args);

			// quad filter expression
			if(g_argv.expression) {
				let g_parse = parse_filter(g_argv.expression);

				let sj_eval = expression_handler.prepare(g_parse);

				if(g_argv.verbose) {
					console.warn(`The compiled quad filter expression from 'transform' command: () => {\n${sj_eval.replace(/^|\n/g, '\n\t')}\n}\n`);
				}

				let f_filter = new Function('factory', 'stream', sj_eval);  // eslint-disable-line no-new-func

				return g_context.inputs.map((ds_input) => {
					let ds_filter = f_filter(factory, stream);

					// intercept pipe
					ds_filter.pipe = F_ADAPT_STREAM;

					// pipe reader to filter
					return ds_input.pipe(ds_filter);
				});
			}
			// javascript expression
			else if(g_argv.javascript) {
				let f_build = new Function('factory', /* syntax: js */ `return (${g_argv.javascript}) || null;`);  // eslint-disable-line no-new-func

				let f_filter = f_build(factory);

				// filter exists
				if(f_filter) {
					// invalid type
					if('function' !== typeof f_filter) {
						exit(`The 'filter' command expects -j/--javascript expression to evaluate to a function, instead found '${typeof f_filter}'`);
					}

					return g_context.inputs.map((ds_input) => {
						let h_prefixes = {};

						let ds_filter = new stream.Transform.QuadsToOther({
							objectMode: true,

							prefix(si_prefix, p_iri) {
								h_prefixes[si_prefix] = p_iri;
							},

							transform(g_quad, s_encoding, fke_transform) {
								if(f_filter(g_quad, h_prefixes)) {
									return fke_transform(null, g_quad);
								}

								fke_transform();
							},
						});

						// intercept pipe
						ds_filter.pipe = F_ADAPT_STREAM;

						// pipe reader to filter
						return ds_input.pipe(ds_filter);
					});
				}
			}

			// neither used (bypass filter)
			warn(`The 'filter' command was not used and is being ignored.`);
			return bypass(g_context.inputs);
		},

		// transform
		transform(a_args, g_context) {
			let g_argv = command(g_context.command)
				.options({
					j: {
						type: 'string',
						alias: ['js', 'javascript'],
						describe: 'transform quads using the given JavaScript expression which will be evaluated as a callback function passed the quad and current prefix map as arguments',
						demandOption: true,
						example: [
							`transform -j 'g => [g.object, g.predicate, g.subject]'`,
							`transform -j 'g => ({
								[factory.blankNode()]: {
									a: 'rdf:Statement',
									'rdf:subject': g.subject,
									'rdf:predicate': g.predicate,
									'rdf:object': g.object,
								},
							})'`,
						].join('\n'),
					},
				})
				.help()
				.parse(a_args);

			// javascript expression
			if(g_argv.javascript) {
				let f_build = new Function('factory', 'c3', 'c4', /* syntax: js */ `return (${g_argv.javascript}) || null;`);  // eslint-disable-line no-new-func

				let f_transform = f_build(factory, factory.c3, factory.c4);

				// transform exists
				if(f_transform) {
					// invalid type
					if('function' !== typeof f_transform) {
						exit(`The 'filter' command expects -j/--javascript expression to evaluate to a function, instead found '${typeof f_filter}'`);
					}

					return g_context.inputs.map((ds_input) => {
						let h_prefixes = {};

						let ds_transform = new stream.Transform.QuadsToOther({
							objectMode: true,

							prefix(si_prefix, p_iri) {
								h_prefixes[si_prefix] = p_iri;
							},

							transform(g_quad, s_encoding, fke_transform) {
								// alias quad property access
								g_quad.s = g_quad.subject;
								g_quad.p = g_quad.predicate;
								g_quad.o = g_quad.object;
								g_quad.g = g_quad.graph;

								// try to apply transform callback
								let z_item;
								try {
									z_item = f_transform(g_quad, h_prefixes);
								}
								catch(e_transform) {
									warn(`The 'transform' command threw an Error while applying the given callback function:\n${e_transform.stack}`);
									return fke_transform();
								}

								// item was returned
								if(z_item) {
									return interpret_item(z_item, h_prefixes, this, fke_transform);
								}

								// done
								fke_transform();
							},
						});

						// intercept pipe
						ds_transform.pipe = F_ADAPT_STREAM;

						// pipe reader to transform
						return ds_input.pipe(ds_transform);
					});
				}
			}

			// nothing used (bypass filter)
			warn(`The 'transform' command was not used and is being ignored.`);
			return bypass(g_context.inputs);
		},


		'content.nt.read': reader(graphy.content.nt.read),
		'content.nq.read': reader(graphy.content.nq.read),
		'content.ttl.read': reader(graphy.content.ttl.read),
		'content.trig.read': reader(graphy.content.trig.read),

		'content.nt.write': writer(graphy.content.nt.write),
		'content.nq.write': writer(graphy.content.nq.write),
		'content.ttl.write': writer(graphy.content.ttl.write),
		'content.trig.write': writer(graphy.content.trig.write),

		'util.dataset.tree': async(a_args, g_context) => {
			const dataset_tree = graphy.util.dataset.tree;

			let s_group_multi_input = 'Transform 1 or more inputs to 1 output:';
			let s_group_dual_input = 'Transform exactly 2 inputs into 1 output:';
			let s_group_boolean = 'Test exactly 2 inputs to get `true` or `false`:';

			let h_operations = {
				u: {
					type: 'boolean',
					alias: ['union'],
					group: s_group_multi_input,
					describe: 'perform the union of 1 or more inputs',
				},
				i: {
					type: 'boolean',
					alias: ['intersect', 'intersection'],
					group: s_group_multi_input,
					describe: 'perform the intersection of 1 or more inputs',
				},
				d: {
					type: 'boolean',
					alias: ['diff', 'difference'],
					group: s_group_dual_input,
					describe: 'perform a difference between two inputs',
				},
				m: {
					type: 'boolean',
					alias: ['minus', 'subtract', 'subtraction'],
					group: s_group_dual_input,
					describe: 'perform a subtraction by removing input-B from input-A',
				},
				c: {
					type: 'boolean',
					alias: ['contains'],
					group: s_group_boolean,
					describe: 'test if input-A completely contains input-B, i.e., if B is a subset of A',
				},
				j: {
					type: 'boolean',
					alias: ['disjoint'],
					group: s_group_boolean,
					describe: 'test if input-A is disjoint with input-B',
				},
				e: {
					type: 'boolean',
					alias: ['equals'],
					group: s_group_boolean,
					describe: 'test if input-A is exactly equal to input-B (you can test for isomorphism by piping thru --canonicalize first)',
				},
			};

			let a_operation_keys = Object.keys(h_operations);
			for(let s_operation of a_operation_keys) {
				h_operations[s_operation].conflicts = a_operation_keys.filter(s => s_operation !== s);
			}

			Object.assign(h_operations, {
				z: {
					type: 'boolean',
					alias: ['canonicalize'],
					group: s_group_multi_input,
					describe: 'canonicalize 1 or more inputs',
				},
			});

			let g_argv = command(g_context.command)
				.options(h_operations)
				.help()
				.version(false)
				.parse(a_args);

			// canonicalize flag
			let b_canonicalize = g_argv.canonicalize;

			// ref inputs; cache length
			let a_inputs = g_context.inputs;
			let n_inputs = a_inputs.length;

			// multi-input stream-output operation
			if(g_argv.union || g_argv.intersection) {
				let s_operation = g_argv.union
					? 'union'
					: 'intersection';

				// // less than 2 inputs; no-op
				// if(n_inputs < 2) return a_inputs;

				// create trees
				let a_trees = a_inputs.map(() => dataset_tree());

				// initial tree
				let k_tree_out = a_trees[0];

				// pairwise readiness
				for(let i_input=0; i_input<n_inputs; i_input++) {
					let k_tree_b = a_trees[i_input];

					// pipe input stream to tree b
					a_inputs[i_input].pipe(k_tree_b);

					// wait for input stream to finish writing to b
					await k_tree_b.until('finish');

					// canonicalize
					if(b_canonicalize) {
						k_tree_b = a_trees[i_input] = k_tree_b.canonicalize();

						// update out ref
						if(!i_input) k_tree_out = k_tree_b;
					}

					// non-first input
					if(i_input) {
						// perform pairwise operation
						k_tree_out = k_tree_out[s_operation](k_tree_b);
					}
				}

				// return readable tree
				return [k_tree_out];
			}
			// dual-input stream-output operation
			else if(g_argv.difference || g_argv.subtraction) {
				let s_operation =  g_argv.difference
					? 'difference'
					: 'minus';

				// not two inputs
				if(2 !== n_inputs) {
					exit(`operation '${s_operation}' expects two inputs but found ${n_inputs}`);
				}

				// async
				return new Promise((fk_resolve) => {
					let operate = () => [k_tree_a[s_operation](k_tree_b)];

					// wait for a
					let k_tree_a = dataset_tree();
					let b_finished_a = false;
					k_tree_a.on('finish', () => {
						// canonicalize
						if(b_canonicalize) k_tree_a = k_tree_a.canonicalize();

						// a is finished now
						b_finished_a = true;

						// b is already finished
						if(b_finished_b) fk_resolve(operate());
					});

					// wait for b
					let k_tree_b = dataset_tree();
					let b_finished_b = false;
					k_tree_b.on('finish', () => {
						// canonicalize
						if(b_canonicalize) k_tree_b = k_tree_b.canonicalize();

						// b is finished now
						b_finished_b = true;

						// a is already finished
						if(b_finished_a) fk_resolve(operate());
					});

					// ref both input streams
					let [ds_input_a, ds_input_b] = a_inputs;

					// pipe each to its tree
					ds_input_a.pipe(k_tree_a);
					ds_input_b.pipe(k_tree_b);
				});
			}
			// boolean
			else if(g_argv.contains || g_argv.disjoint || g_argv.equals) {
				let s_operation =  g_argv.contains
					? 'contains'
					: (g_argv.disjoint
						? 'disjoint'
						: 'equals');

				// not two inputs
				if(2 !== n_inputs) {
					exit(`boolean operation '${s_operation}' expects two inputs but found ${n_inputs}`);
				}

				// async
				return new Promise((fk_resolve) => {
					let operate = () => [new answer_source(k_tree_a[s_operation](k_tree_b))];

					// wait for a
					let k_tree_a = dataset_tree();
					let b_finished_a = false;
					k_tree_a.on('finish', () => {
						// canonicalize
						if(b_canonicalize) k_tree_a = k_tree_a.canonicalize();

						// a is finished now
						b_finished_a = true;

						// b is already finished
						if(b_finished_b) fk_resolve(operate());
					});

					// wait for b
					let k_tree_b = dataset_tree();
					let b_finished_b = false;
					k_tree_b.on('finish', () => {
						// canonicalize
						if(b_canonicalize) k_tree_b = k_tree_b.canonicalize();

						// b is finished now
						b_finished_b = true;

						// a is already finished
						if(b_finished_a) fk_resolve(operate());
					});

					// ref both input streams
					let [ds_input_a, ds_input_b] = a_inputs;

					// pipe each to its tree
					ds_input_a.pipe(k_tree_a);
					ds_input_b.pipe(k_tree_b);
				});
			}
			// map; n-to-n
			else {
				return g_context.inputs.map(ds_input => ds_input.pipe(dataset_tree({
					canonicalize: g_argv.canonicalize,
				})));
			}
		},
	};

	// command aliases
	h_commands.read = h_commands['content.trig.read'];
	h_commands.write = writer(graphy.content.trig.write, {
		tokens: {
			simplify_default_graph: true,
		},
	});
	h_commands.tree = h_commands['util.dataset.tree'];

	let a_argv = process.argv.slice(2);
	let n_args = a_argv.length;

	// no arguments
	if(!a_argv.length) {
		exit('no arguments given');
	}

	// inputs
	let a_inputs = [];

	// pipeline
	let a_pipeline = [];
	{
		let a_series = [];

		for(let i_argv=0; i_argv<n_args; i_argv++) {
			let s_arg = a_argv[i_argv];

			// after first arg
			if(i_argv) {
				// internal pipe
				if('--pipe' === s_arg) {
					a_pipeline.push(a_series);
					if(i_argv === n_args) {
						exit(`was expecting pipe destination after --pipe: ${a_argv}`);
					}
					a_series = [];
					continue;
				}
				// shorthand internal pipe
				else if('/' === s_arg) {
					a_pipeline.push(a_series);
					if(i_argv === n_args) {
						exit(`was expecting pipe destination after internal pipe character '/': ${a_argv}`);
					}
					a_series = [];
					continue;
				}
				// inputs follow
				else if('--inputs' === s_arg) {
					// convert to readable streams
					a_inputs.push(...a_argv.slice(i_argv+1).map(p => fs.createReadStream(p)));
					break;
				}
			}
			// first arg
			else {
				// main option
				if('-h' === s_arg || '--help' === s_arg) {
					// eslint-disable-next-line no-console
					console.log('\n'+gobble(`
						Usage: graphy COMMAND [--pipe COMMAND]*

						Commands:
						  filter                Apply a filter to the stream of quads
						  content.nt.read       Read an N-Triples document
						  content.nt.write      Write to N-Triples format
						  content.nq.read       Read an N-Quads document
						  content.nq.write      Write to N-Quads format
						  content.ttl.read      Read a Turtle document
						  content.ttl.write     Write to Turtle format
						  content.trig.read     Read a TriG document
						  content.trig.write    Write to TriG format
						  util.dataset.tree     Perform some transformation on a datset

						Command aliases:
						  read     -> content.trig.read
						  write    -> content.trig.write
						  tree     -> util.dataset.tree

						Run 'graphy COMMAND --help' for more information on a command.
					`));
					process.exit(0);
				}
				// version
				else if('-v' === s_arg || '--version' === s_arg) {
					// eslint-disable-next-line no-console
					console.log(require(path.join(__dirname, './package.json')).version);
					process.exit(0);
				}
			}

			a_series.push(s_arg);
		}

		// empty series
		if(a_series.length) {
			a_pipeline.push(a_series);
		}
	}

	// empty command list
	if(!a_pipeline.length) {
		exit('no commands given');
	}

	(async() => {
		// failure handler
		let f_failure = (e_command) => {
			exit(e_command.message);
		};

		// starting inputs default to stdin if no explicit inputs given
		let a_prev = a_inputs.length? a_inputs: [process.stdin];

		// each series in pipeline
		for(let a_series of a_pipeline) {
			// start with command string
			let s_command = a_series[0];

			// no such command
			if(!(s_command in h_commands)) {
				exit(`no such command '${s_command}'`);
			}

			try {
				// eval command with its args
				let a_curr = await h_commands[s_command](a_series.slice(1), {
					command: s_command,
					inputs: a_prev,
					failure: f_failure,
				});

				// advance inputs
				a_prev = a_curr;
			}
			catch(e_command) {
				exit(e_command.message);
			}
		}

		// expect single output
		if(1 !== a_prev.length) {
			exit(`expected a single output stream but last command produces ${a_prev.length} streams`);
		}

		// pipe output to stdout
		a_prev[0].pipe(process.stdout);
	})();
}
